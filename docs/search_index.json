[
["index.html", "Engaging the Web with R", " Engaging the Web with R Michael Clark https://m-clark.github.io/ 2018-08-12 "],
["intro.html", "Introduction Prerequisites", " Introduction Well known for its statistical capabilities, R can also be used for web-scraping, connecting with websites via APIs, html documents and presentations, interactive visualizations, dashboards, and even building entire, possibly interactive, websites. This document and related talk will provide an overview of web-based use of R. Conceptual introductions, package synopses, and small demonstrations will be presented. Prerequisites Basic R knowledge is all that is required, and not even statistical knowledge of any kind is assumed. The focus will be on breadth (e.g. common use) rather than depth. One of the things you’ll want to do before getting started is to peruse the Task Views for Web Technologies. There is heavy use of the tidyverse for data processing, so you’ll need to be familiar with it to understand what the code is doing. Color coding: emphasis package function object/class link "],
["web_scraping.html", "Web scraping Direct data download Key concepts The basic approach Examples Issues", " Web scraping We begin with a discussion on web scraping. The term itself is ambiguous, and could potentially mean anything1, but the gist is that there is something out on the web (or some connected machine) that we want, and we’d like to use R to get it. This section won’t (at least for now) get into lower level utilities such as that provided by httr or Rcurl, though some packages will be using them under the hood. Instead, focus will be on higher-level approaches with an eye toward common tasks. As a starting point, open a browser and go to a website of your preference. For most browsers, Ctrl+U will open up the underlying html file. If you’re on a typical webpage it will almost certainly look like a mess of html, JavaScript, XML and possibly other things. Simpler pages are more easily discernible, while more complex/secure pages are not. The take home message is that what you want is represented by something in there, and you’ll have to know something about that structure in order to get it. Unfortunately, a lot of web design is quite poor2, which will make your job difficult. Even when the sole purpose of a site is to provide data, you can almost be sure that the simplest/most flexible path will not be taken to do so. Thankfully you can use R and possibly other tools to make the process at least somewhat less painful. Direct data download One thing to be aware of is that you can get data from the web that are in typical formats just as you would any file on your own machine. For example: mydata = read.csv(&#39;http://somewebsite/data.csv&#39;) I wouldn’t even call this web scraping, just as you wouldn’t if you were getting a file off a network connection. However, if you’re just starting out with R, it’s important to know this is possible. In any case, do not make things more difficult than they need to be- if the file is available just grab it. Key concepts The web works in mysterious ways, but you don’t have to know a lot about it to use it or benefit from its data. Many people have had their own websites for years without knowing any html. For our purposes, it helps to know a little and is actually required, at least so that we know what to look for. Common elements or tags3 of a webpage include things like div table and ul and body, and within such things our data will be found. Some common ones include: div and span: used when other elements are not appropriate but one still wants to define classes, ids etc. p paragraph a link (technically ‘anchor’) ul ol lists table tr td tables, table rows, table data (cell) h# i.e. h1 h2 etc. headers img images Consider the following: &lt;h3&gt; This header denotes a section of text &lt;img src=&quot;picture_of_cat.png&quot;&gt;&lt;/img&gt; &lt;p&gt; This is a paragraph! &lt;/p&gt; &lt;p&gt; Here is another! This one has &lt;a href=&quot;www.someurl.com&quot;&gt;a link&lt;/a&gt;! &lt;/p&gt; As an example, if we wanted to scrape every paragraph that had the word paragraph in it, we’d need to first get all the &lt;p&gt; elements and work from there. Just like in other programming languages, these parts of a webpage have their own class, e.g. &lt;table class=&quot;wikitable sortable&quot;&gt;, and this allows even easier access to the objects of specific interest. Take a look at the source for the Wikipedia page for towns in Michigan. The id is another attribute, or way to note specific types of objects. Unlike classes, which might be used over and over within a page, ids are unique, and thus for web scraping, probably more useful when appropriate to the task. Also, while elements can have multiple classes, they can have only one id. Any particular element might also have different attributes that specify things like style, alignment, color, file source etc. The sad thing is that attributes are greatly underutilized in general. For example, perhaps you found a page with several html tables of data, one for each year. It would have been simple to add an id = year to each one, enabling you to grab just the specific year(s) of interest, but it’ll be rare that you’d find a page that goes that far. Part of this could be due to the fact that much content is auto-generated via WYSIWYG editors, but these are not very professional pages if so, which might cause concern for the data given such a source. Take a look at the open.fda.gov site4: It has lots of attributes for practically every element, which would make getting stuff off the page fairly easy, if it didn’t already have an API. In any case, you’ll at least need to know the tag within which the data or link to it may be found. Classes and ids will help to further drill down the web page content, but often won’t be available. In such a case you might have to grab, for example, all the links, and then use some regular expression to grab only the one you want. See this link for more on relations between attributes and elements. The basic approach To summarize the basic approach, we can outline a few steps: If a direct link or API is available use it5 Inspect the page to find the relevant tags within which the content is found Start with a base URL Use the relevant R packages to parse the base page Extract the desired content Examples One of the packages that can make scraping easy is rvest, which is modeled after/inspired by the Beautiful Soup module in Python6. As the goal here is to get you quickly started, we won’t inundate you with a lot of packages yet. I recommend getting comfy with rvest and moving to others when needed. Tables Wikipedia A lot of Wikipedia has pages ripe for the scraping, but it also has an API, so that we can use it as an example later as well. Between the page layout and rvest it will be very easy to get the content. So let’s do so. Back to the page of towns in Michigan. Let’s see if we can get the table of towns with their type, county, and population. First things first, we need to get the page. page = &#39;https://en.wikipedia.org/wiki/List_of_cities,_villages,_and_townships_in_Michigan&#39; library(rvest) towns = read_html(page) str(towns) List of 2 $ node:&lt;externalptr&gt; $ doc :&lt;externalptr&gt; - attr(*, &quot;class&quot;)= chr [1:2] &quot;xml_document&quot; &quot;xml_node&quot; towns {xml_document} &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;\\n&lt;meta charset=&quot;UTF-8&quot;&gt;\\n&lt;title&gt;List of cities, villages, and townships in Michigan - Wikipedia&lt;/title&gt;\\n&lt;script&gt;document.documentElement ... [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-List_of_cities_villages_and_townships_in_Michigan rootpage-List_of_cities_villages_and_townships_in_Michigan skin-vector action-view&quot;&gt;\\ ... The result of read_html is an xml_document class object. For the uninitiated, XML is a markup language (Extensible Markup Language) like HTML, and which allows one to access its parts as nodes in tree7, where parents have children and grandchildren etc. It will require further parsing in order to get what we want, but it was easy enough to snag the page. Let’s look at some of the nodes. html_nodes(towns, &#39;ul&#39;) %&gt;% head # unordered lists (first few) {xml_nodeset (6)} [1] &lt;ul&gt;\\n&lt;li&gt;&lt;a href=&quot;#A&quot;&gt;A&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#B&quot;&gt;B&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#C&quot;&gt;C&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#D&quot;&gt;D&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#E&quot;&gt;E&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#F&quot;&gt;F&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#G&quot;&gt;G&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt; ... [2] &lt;ul&gt;\\n&lt;li&gt;&lt;a href=&quot;#See_also&quot;&gt;See also&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#References&quot;&gt;References&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#External_links&quot;&gt;External links&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt; [3] &lt;ul&gt;\\n&lt;li&gt;&lt;a href=&quot;/wiki/Administrative_divisions_of_Michigan&quot; title=&quot;Administrative divisions of Michigan&quot;&gt;Administrative divisions of Michigan&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;/wiki/List_of_counties_in_Michigan&quot; title=&quot;Li ... [4] &lt;ul&gt;\\n&lt;li&gt;&lt;a rel=&quot;nofollow&quot; class=&quot;external text&quot; href=&quot;https://web.archive.org/web/20040908085409/http://www.viennatwp.com/News/township_government_in_michigan.htm&quot;&gt;Township Government in Michigan&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt; ... [5] &lt;ul&gt;\\n&lt;li&gt;&lt;a rel=&quot;nofollow&quot; class=&quot;external text&quot; href=&quot;https://web.archive.org/web/20020802223743/http://www.census.gov/geo/www/gazetteer/places2k.html&quot;&gt;Census 2000 Gazetteer&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a rel=&quot;nofollow&quot; class ... [6] &lt;ul&gt;\\n&lt;li class=&quot;nv-view&quot;&gt;&lt;a href=&quot;/wiki/Template:Michigan&quot; title=&quot;Template:Michigan&quot;&gt;&lt;abbr title=&quot;View this template&quot; style=&quot;;;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none; ... html_nodes(towns, &#39;a&#39;) %&gt;% head # links {xml_nodeset (6)} [1] &lt;a id=&quot;top&quot;&gt;&lt;/a&gt; [2] &lt;a class=&quot;mw-jump-link&quot; href=&quot;#mw-head&quot;&gt;Jump to navigation&lt;/a&gt; [3] &lt;a class=&quot;mw-jump-link&quot; href=&quot;#p-search&quot;&gt;Jump to search&lt;/a&gt; [4] &lt;a href=&quot;/wiki/Wikipedia:Citing_sources&quot; title=&quot;Wikipedia:Citing sources&quot;&gt;list of references&lt;/a&gt; [5] &lt;a href=&quot;/wiki/Wikipedia:Citing_sources#Inline_citations&quot; title=&quot;Wikipedia:Citing sources&quot;&gt;inline citations&lt;/a&gt; [6] &lt;a href=&quot;/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check&quot; title=&quot;Wikipedia:WikiProject Fact and Reference Check&quot;&gt;improve&lt;/a&gt; html_nodes(towns, &#39;table&#39;) # tables {xml_nodeset (5)} [1] &lt;table class=&quot;plainlinks metadata ambox ambox-style ambox-More_footnotes&quot; role=&quot;presentation&quot;&gt;&lt;tbody&gt;&lt;tr&gt;\\n&lt;td class=&quot;mbox-image&quot;&gt;&lt;div style=&quot;width:52px&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;//upload.wikimedia.org/wikipedia/commons/t ... [2] &lt;table class=&quot;wikitable sortable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Place&lt;/th&gt;\\n&lt;th&gt;Type&lt;/th&gt;\\n&lt;th&gt;County&lt;/th&gt;\\n&lt;th&gt;2010 Population&lt;/th&gt;\\n&lt;th&gt;2017 Estimate\\n&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;tr id=&quot;A&quot;&gt;\\n&lt;td&gt;&lt;a href=&quot;/wiki/Acme,_Michigan&quot; class=&quot;mw ... [3] &lt;table role=&quot;presentation&quot; class=&quot;mbox-small plainlinks sistersitebox&quot; style=&quot;background-color:#f9f9f9;border:1px solid #aaa;color:#000&quot;&gt;&lt;tbody&gt;&lt;tr&gt;\\n&lt;td class=&quot;mbox-image&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;//upload.wikimedia.org/ ... [4] &lt;table class=&quot;nowraplinks collapsible autocollapse navbox-inner&quot; style=&quot;border-spacing:0;background:transparent;color:inherit&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;&lt;th scope=&quot;col&quot; class=&quot;navbox-title&quot; colspan=&quot;3&quot;&gt;\\n&lt;div class=&quot;plainlinks ... [5] &lt;table class=&quot;nowraplinks collapsible autocollapse navbox-inner&quot; style=&quot;border-spacing:0;background:transparent;color:inherit&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;&lt;th scope=&quot;col&quot; class=&quot;navbox-title&quot; colspan=&quot;2&quot;&gt;\\n&lt;div class=&quot;plainlinks ... Perhaps now you are getting a sense of what we can possibly extract from this page. It turns out that what we want is a table element, and in particular, the one that is of class wikitable sortable. This will make things very easy. But what if we didn’t know what the element was? it turns out browsers come with the ability to inspect the elements of any webpage, and even change them. The following depicts what the Michigan towns looks like when using Chrome’s built-in developer tools and selecting a link. It highlights everything else in the page that is of a similar type (i.e. other links), as well as provides the css/xml info that we would need to grab that particular item. As mentioned, we want the wikitable sortable class table, so lets grab that. The rvest package comes with the html_table function, that will grab any table and attempt to put it into a data.frame object. I only show the first couple because one of the tables comes out pretty messy at the R console. str(html_table(towns, fill = TRUE)[1:3]) List of 3 $ :&#39;data.frame&#39;: 1 obs. of 2 variables: ..$ X1: logi NA ..$ X2: chr &quot;This article includes a list of references, but its sources remain unclear because it has insufficient inline c&quot;| __truncated__ $ :&#39;data.frame&#39;: 2188 obs. of 5 variables: ..$ Place : chr [1:2188] &quot;Acme&quot; &quot;Acme Township&quot; &quot;Ada Township&quot; &quot;Adams Township, Arenac County&quot; ... ..$ Type : chr [1:2188] &quot;unincorporated community&quot; &quot;township&quot; &quot;township&quot; &quot;township&quot; ... ..$ County : chr [1:2188] &quot;Grand Traverse&quot; &quot;Grand Traverse&quot; &quot;Kent&quot; &quot;Arenac&quot; ... ..$ 2010 Population: chr [1:2188] &quot;&quot; &quot;4,375&quot; &quot;13,142&quot; &quot;563&quot; ... ..$ 2017 Estimate : logi [1:2188] NA NA NA NA NA NA ... $ :&#39;data.frame&#39;: 1 obs. of 2 variables: ..$ X1: logi NA ..$ X2: chr &quot;Wikimedia Commons has media related to Populated places in Michigan.&quot; We know which class object we want though, so we could have used html_nodes to grab only that object and work with it. html_nodes(towns, &#39;.wikitable.sortable&#39;) %&gt;% html_table(fill = TRUE) %&gt;% .[[1]] %&gt;% head() Place Type County 2010 Population 2017 Estimate 1 Acme unincorporated community Grand Traverse NA 2 Acme Township township Grand Traverse 4,375 NA 3 Ada Township township Kent 13,142 NA 4 Adams Township, Arenac County township Arenac 563 NA 5 Adams Township, Hillsdale County township Hillsdale 2,493 NA 6 Adams Township, Houghton County township Houghton 2,573 NA Here the . in .wikitable.sortable represents the class/subclass (use # for ids). In general though, if you know it’s a table, then use html_table. When the table has no class then you’ll have to use a magic number or some other means to grab it, which may not be as robust. Of course, the class itself might change over time also. So let’s put this all together. library(stringr) towns %&gt;% html_table(fill = TRUE) %&gt;% .[[2]] %&gt;% rename(Population = `2010 Population`) %&gt;% mutate(Population = strtoi(str_replace_all(Population, &#39;,&#39;, &#39;&#39;)), Type = factor(str_squish(str_trim(Type))), Type = forcats::fct_collapse(Type, unincorporated = c(&#39;unincorporated community&#39;, &#39;unincorporated community and CDP&#39;, &#39;CDP&#39;))) %&gt;% ggplot(aes(x=Population, fill=Type, color=Type)) + scale_x_log10() + geom_density(alpha=.2) + theme(panel.background=element_rect(fill=&#39;transparent&#39;, color=NA), plot.background=element_rect(fill=&#39;transparent&#39;, color=NA)) I won’t go into details about the rest of the code regarding data processing, as that’s for another workshop. For now it suffices to say that it didn’t take much to go from URL to visualization. Basketball Reference As an additional example let’s get some data from basketball-reference.com. The following gets the totals table from the URL8. Issues include header rows after every 20th row of data, and converting all but a few columns from character to numeric. url = &quot;http://www.basketball-reference.com/leagues/NBA_2018_totals.html&quot; bball = read_html(url) %&gt;% html_node(&quot;#totals_stats&quot;) %&gt;% # grab element with id &#39;total_stats&#39; html_table() %&gt;% # the data filter(Rk != &quot;Rk&quot;) %&gt;% # remove header rows mutate_at(vars(-Player, -Pos, -Tm), as.numeric) # convert to numeric str(bball) &#39;data.frame&#39;: 664 obs. of 30 variables: $ Rk : num 1 2 3 4 5 6 7 8 9 10 ... $ Player: chr &quot;Alex Abrines&quot; &quot;Quincy Acy&quot; &quot;Steven Adams&quot; &quot;Bam Adebayo&quot; ... $ Pos : chr &quot;SG&quot; &quot;PF&quot; &quot;C&quot; &quot;C&quot; ... $ Age : num 24 27 24 20 32 29 32 19 25 36 ... $ Tm : chr &quot;OKC&quot; &quot;BRK&quot; &quot;OKC&quot; &quot;MIA&quot; ... $ G : num 75 70 76 69 53 21 75 72 18 22 ... $ GS : num 8 8 76 19 3 0 75 31 1 0 ... $ MP : num 1134 1359 2487 1368 682 ... $ FG : num 115 130 448 174 65 5 687 234 6 44 ... $ FGA : num 291 365 712 340 162 ... $ FG% : num 0.395 0.356 0.629 0.512 0.401 0.333 0.51 0.589 0.273 0.484 ... $ 3P : num 84 102 0 0 27 0 27 5 0 4 ... $ 3PA : num 221 292 2 7 70 0 92 15 11 12 ... $ 3P% : num 0.38 0.349 0 0 0.386 NA 0.293 0.333 0 0.333 ... $ 2P : num 31 28 448 174 38 5 660 229 6 40 ... $ 2PA : num 70 73 710 333 92 ... $ 2P% : num 0.443 0.384 0.631 0.523 0.413 0.333 0.526 0.599 0.545 0.506 ... $ eFG% : num 0.54 0.496 0.629 0.512 0.485 0.333 0.52 0.596 0.273 0.505 ... $ FT : num 39 49 160 129 22 2 334 114 7 11 ... $ FTA : num 46 60 286 179 26 6 399 147 9 21 ... $ FT% : num 0.848 0.817 0.559 0.721 0.846 0.333 0.837 0.776 0.778 0.524 ... $ ORB : num 26 40 384 118 4 3 246 144 4 20 ... $ DRB : num 88 217 301 263 62 12 389 244 7 27 ... $ TRB : num 114 257 685 381 66 15 635 388 11 47 ... $ AST : num 28 57 88 101 30 3 152 49 12 9 ... $ STL : num 38 33 92 32 4 2 43 28 3 11 ... $ BLK : num 8 29 78 41 9 1 90 88 2 3 ... $ TOV : num 25 60 128 66 21 1 111 82 9 19 ... $ PF : num 124 149 215 138 56 11 161 147 15 49 ... $ PTS : num 353 411 1056 477 179 ... Text A lot of times we’ll want to grab text as opposed to tables. See the API chapter for an example, and the html_text function in the rvest package. Images Images are fairly easy because they are simply files with specific extensions like svg, png, gif etc. In that sense, if one knows the actual location of the image already, a function like download.files can be used to grab it directly. Other times it may not be known, and so we can use a similar approach as before to grab the file. base_page = read_html(&#39;https://en.wikipedia.org/wiki/Main_Page&#39;) picofday_location = base_page %&gt;% # get the main page html_nodes(&#39;#mp-tfp&#39;) %&gt;% # extract the div with &#39;today&#39;s featured picture&#39;, i.e. tfp html_nodes(&#39;img&#39;) %&gt;% # extract the img html_attr(&#39;src&#39;) # grab the source location With location in hand we can now download the file, and even display it in R. The following requires the grid and jpeg packages. download.file(url=paste0(&#39;https:&#39;, picofday_location), destfile=&#39;img/picofday.jpg&#39;, mode=&#39;wb&#39;) picofday = jpeg::readJPEG(source=&#39;img/picofday.jpg&#39;) df = data.frame(x=rnorm(1), y=rnorm(1)) # note that any random df will suffice qplot(data=df, geom=&#39;blank&#39;) + annotation_custom(grid::rasterGrob(picofday)) + theme_void() Note that an alternative approach that might work on some websites would be to extract all the images and then the one with a relevant naming convention. This doesn’t work for the Wikipedia main page because there is nothing to identify which image is the featured picture by file name alone. Issues As mentioned, the ease with which you will be able to scrape a website will depend a lot on how well the page/site is put together. Many are cookie-cutter templates designed with no regard to data availability whatsoever, others are just the result of amateurs (like me!) that do not do web design for a living, and still others are just poorly designed. In addition, some websites may have security or other server back end things to consider that require much of the content to be less accessible, perhaps intentionally. On the other hand, other sites will make it as easy as a URL pointing to a csv file, or an API that is easily maneuverable. The main thing is that you must plan ahead for encountering difficulty, and/or to be ready for heavy post-processing even when you get the bulk of what you want. The goal should be to make the process automatic such that only a fundamental site or API change will cause you to have to change your code again. Called by various names: web harvesting, data scraping (presumptuous), etc.↩ Think about your R code, now remove the spaces, indent as irregularly as possible, do not comment anything, use reserved words for common names, don’t name important parts of your site, reject any known coding style, and use default settings. That would describe much of the typical website design I come across, and the rest is automatically generated, so possibly not even intended to be human-readable.↩ There is subtle distinction between tags versus elements, but which won’t really matter to us.↩ The openfda site was depicted using the Mozilla firebug extension.↩ APIs change so regularly that in some cases it might be easier to scrape as above, especially if the site itself changes relatively rarely and still allows direct access to the desired content.↩ I highly recommend Beautiful Soup also. Until rvest and related packages came along, I preferred using Beautiful Soup to what was available in R.↩ Think graphical models rather than spruce.↩ Technically this data can be downloaded as a supplied csv.↩ "],
["APIs.html", "APIs Raw Example R packages Issues", " APIs An application programming interface allows one to interact with a website9. In the simplest situation, this is merely a url-based approach to grabbing what you need. As an example consider the following generic URL: http://somewebsite.com/key?param_1;param_2 The key ingredients are: http://somewebsite.com/ the base URL key some authorization component ?param_1;param_2 the parameters of interest that specify what you want to grab from that website. With R, once we have authorization we can then simply feed the parameters that tell the server what data to provide. We can do this in a raw fashion, where we make the URL, or web address10, that has the necessary specification, and then just take what it provides. Alternatively, there are many R packages to make the process easier for things like Twitter, Qualtrics, and many other websites. Raw Example Basic functions of the raw approach are requests to a server such as GET and POST, commands that tell the server to provide something or perhaps provide data to it. In the following, api_key11 is an R object with a character string of the key provided to me by the website. I use the httr package for the web functionality to acquire the content. In particular, the GET function retrieves whatever information is noted by the request/url. Additional arguments or modifications to the base URL can also be provided, which is what the query part does. # raw approach library(httr) most_viewed_base = GET(&#39;https://api.nytimes.com/svc/mostpopular/v2/mostviewed/all-sections/30.json&#39;, query=list(`api-key`=api_key)) # see the url created # most_viewed_base$url # tacks on ?api-key=YOURAPI at the end At this point, most_viewed_base is a response class object, a list with several pieces of information including what we want, which is the content. str(most_viewed_base[-1], 1) List of 9 $ status_code: int 200 $ headers :List of 18 ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;insensitive&quot; &quot;list&quot; $ all_headers:List of 1 $ cookies :&#39;data.frame&#39;: 0 obs. of 7 variables: $ content : raw [1:59907] 7b 22 73 74 ... $ date : POSIXct[1:1], format: &quot;2018-08-12 15:38:04&quot; $ times : Named num [1:6] 0 0.154 0.212 0.309 0.465 ... ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;redirect&quot; &quot;namelookup&quot; &quot;connect&quot; &quot;pretransfer&quot; ... $ request :List of 7 ..- attr(*, &quot;class&quot;)= chr &quot;request&quot; $ handle :Class &#39;curl_handle&#39; &lt;externalptr&gt; However, the content is in binary, and to get it into a useful state we’ll use the content function. The rest of the code just takes the title and arranges it by date. most_viewed = content(most_viewed_base) most_viewed$results %&gt;% map_df(function(x) data_frame(title=x$title, date=x$published_date)) %&gt;% arrange(desc(date)) # A tibble: 20 x 2 title date &lt;chr&gt; &lt;chr&gt; 1 Ohio Special Election Results: 12th Congressional District 2018-08-07 2 Michigan Primary Election Results 2018-08-07 3 The Age That Women Have Babies: How a Gap Divides America 2018-08-04 4 Rick Genest, Tattooed ‘Zombie Boy’ in Lady Gaga Video, Dies at 32 2018-08-03 5 Apple’s Value Hit $1 Trillion. Add Disney to Bank of America and ... You’re Halfway There 2018-08-02 6 Losing Earth: The Decade We Almost Stopped Climate Change 2018-08-01 7 Facebook Identifies an Active Political Influence Campaign Using Fake Accounts 2018-07-31 8 Trump Administration Mulls a Unilateral Tax Cut for the Rich 2018-07-30 9 Motherhood in the Age of Fear 2018-07-27 10 An Extremely Detailed Map of the 2016 Presidential Election 2018-07-25 11 How Goop’s Haters Made Gwyneth Paltrow’s Company Worth $250 Million 2018-07-25 12 Spotting CNN on a TV Aboard Air Force One, Trump Rages Against Reality 2018-07-24 13 When We Eat, or Don’t Eat, May Be Critical for Health 2018-07-24 14 U.S. Breaks Up Vast I.R.S. Phone Scam 2018-07-23 15 Michael Cohen Secretly Taped Trump Discussing Payment to Playboy Model 2018-07-20 16 From the Start, Trump Has Muddied a Clear Message: Putin Interfered 2018-07-18 17 Mariia Butina, Who Sought ‘Back Channel’ Meeting for Trump and Putin, Is Charged as Russian Agent 2018-07-16 18 Trump, Treasonous Traitor 2018-07-15 19 Trump Walks in Front of Queen Elizabeth, Causing Social Media Frenzy 2018-07-14 20 The Power of Positive People 2018-07-10 The community API can get user comments and movie reviews. In the following case, it will need a specific date to retrieve comments. comments_base = GET(&#39;http://api.nytimes.com/svc/community/v3/user-content/by-date.json&#39;, query=list(date=Sys.Date(), `api-key`=api_key)) comments = content(comments_base) comments$results$comments %&gt;% map(function(x) x$commentBody) %&gt;% head() [[1]] [1] &quot;One of the easiest Sundays. This is the only one that I can recall where I was able to not go through multiple passes through the whole puzzle. Solved the top two thirds and never looked back. The bottom third gave a little struggle but fell quickly. &lt;br/&gt;&lt;br/&gt;Loved the theme and the puns. Very entertaining. Figured out the theme after BUTTER RIVALS and CURSES FOULED AGAIN. And, pangram to boot! A very well crafted puzzle. &quot; [[2]] [1] &quot;Paul Ryan has been an abject failure in politics. Not sure that he accomplished much except to show himself to be a weak, impotent, mouthpiece for the greedy and wealthy who sponsored his 20 plus years in congress. A true enemy of the poor, elderly, and under-served. Self serving politicians like Paul Ryan find a way to exploit their time in office. He&#39;ll be fine. Lobbyist?, Fox News contributor?, Conservative \\&quot;think tank\\&quot;?, 2020 presidential candidate????&quot; [[3]] [1] &quot;You betcha. It&#39;s like we all moved into a city that has agreed that all buildings painted in the same base and trim Pantone colors, for nearly 2 decades. &lt;br/&gt;&lt;br/&gt;I&#39;m a performing and recording musician, working with several different bands. The homogeneity has made learning new material almost an oxymoron. The result, as a listener, is that all of the music becomes background noise - with no variance, there isn&#39;t much reason to tune in to any particular song, unless I prefer Taylor Swift&#39;s pout to Katy Perry&#39;s. But - that&#39;s the video...I don&#39;t even need the sound on to appreciate the visual.&lt;br/&gt;&lt;br/&gt;That homogeneity is probably the biggest reason I gave up on pleasure listening to current pop stations over the last 10 years or so. &lt;br/&gt;&lt;br/&gt;Please - let us hear something different, worth getting excited about! Something that I can&#39;t play with 95%+ accuracy after hearing it one time! Give me a good reason to spend less time listening to great music that is 25-50 years old, and listen to something new because it&#39;s interesting.&quot; [[4]] [1] &quot;what Trump knows best, and there&#39;s not a whole lot he knows best, is self-promotion. and to his good fortune, his followers ain&#39;t too smart to see through it. well, good for him.&quot; [[5]] [1] &quot;This article is spot on. I was asked to give a talk once, on the most important skill I had learned that paid off in my career. I spoke of learning to type and noted many of Mr. Bruni&#39;s points...typing was persistence, practice, and eventually success. Such a simple recipe, really...!&quot; [[6]] [1] &quot;@David Appell When money goes out, it may or may not end up paying foreign wages. It could also just go into foreign profit, taxes, real estate, etc.&lt;br/&gt;Eventually, all the money which goes out has to come back. When it comes back to us, it goes to labor, profit, taxes, real estate, etc.&quot; The API documentation doesn’t make obvious how the comments are chosen (aside from being taken from some section of the paper), or what the limits are. As mentioned previously, this is typical API documentation in my experience, such that it might take some effort to know exactly what you’re working with. R packages Many R packages allow extremely easy access to various websites through their API. Usually all it takes is acquiring the authorization and the package will do the rest. It might only be marginally less effort than the raw approach we did before, but can make things more efficient in the long run (assuming the package is kept up to date). New York Times Article Search The following is an example of accessing the New York Times article search API12. Using the rtimes package, we can use a function similar to any other in R. In this case we need a query, beginning date and an end date to collect articles that contain the text of the query. Note that it still requires an key, but that’s not shown here. library(rtimes) article_search = as_search(q=&quot;bomb&quot;, begin_date = &#39;20160918&#39;, end_date = &#39;20160919&#39;)[-(1:2)] article_search$data %&gt;% arrange(desc(pub_date)) # A tibble: 10 x 25 web_url snippet source multimedia keywords pub_date document_type news_desk type_of_material `_id` word_count score print_page uri section_name headline.main headline.kicker headline.conten… headline.print_… &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; 1 https:… U.S. i… &quot;&quot; &lt;data.fra… &lt;data.f… 2016-09… article None News 57df… 0 2.30e-3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; U.S. Officia… NA NA U.S. Officials … 2 https:… &quot;Democ… &quot;&quot; &lt;data.fra… &lt;data.f… 2016-09… article None News 57de… 0 2.91e-5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Clinton Cond… NA NA Clinton Condemn… 3 https:… The au… The N… &lt;data.fra… &lt;data.f… 2016-09… multimedia N.Y. / R… Video 57de… 28 3.07e-5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Explosion Ro… NA NA &lt;NA&gt; 4 https:… The La… &quot;&quot; &lt;data.fra… &lt;data.f… 2016-09… article None News 57de… 0 1.90e-5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; The Latest: … NA NA The Latest: Sus… 5 https:… The pe… The N… &lt;data.fra… &lt;data.f… 2016-09… article Metro News 57de… 1587 2.19e-3 1 nyt:… &lt;NA&gt; Manhattan Bo… NA NA Bombs Built for… 6 https:… Somali… &quot;&quot; &lt;data.fra… &lt;data.f… 2016-09… article None News 57de… 0 2.50e-5 &lt;NA&gt; &lt;NA&gt; Africa Al Shabaab K… NA NA Al Shabaab Kill… 7 https:… A Soma… &quot;&quot; &lt;data.fra… &lt;data.f… 2016-09… article None News 57de… 0 1.97e-3 &lt;NA&gt; &lt;NA&gt; Africa Somali Islam… NA NA Somali Islamist… 8 https:… &quot;Donal… &quot;&quot; &lt;data.fra… &lt;data.f… 2016-09… article None News 57de… 0 1.99e-5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Trump, Clint… NA NA Trump, Clinton … 9 https:… Invest… &quot;&quot; &lt;data.fra… &lt;data.f… 2016-09… article None News 57de… 0 1.98e-5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Police Searc… NA NA Police Search f… 10 https:… Ms. Ke… The N… &lt;data.fra… &lt;data.f… 2016-09… article Arts&amp;Lei… News 57da… 1226 2.11e-3 12 &lt;NA&gt; Television Ellie Kemper… NA NA … I Bombed on L… # ... with 6 more variables: headline.name &lt;lgl&gt;, headline.seo &lt;lgl&gt;, headline.sub &lt;lgl&gt;, byline.original &lt;chr&gt;, byline.person &lt;list&gt;, byline.organization &lt;chr&gt; Wikipedia With the WikipediR package, getting the whole content of a page is just one possibility, and it feeds nicely into rvest functionality for more processing. Here, the package’s page_content function will extract information from the page. The ‘text’ element is the html content, which we can then feed to the previously used rvest functions. I show only a snippet of the output. library(WikipediR) r_wikipedia = page_content(language=&#39;en&#39;, &#39;wikipedia&#39;, page_name = &#39;R_(programming_language)&#39;) str(r_wikipedia[[1]], 1) # inspect r_wikipedia$parse$text$`*` %&gt;% read_html %&gt;% html_text %&gt;% message Qualtrics Qualtrics is a survey software which one can use to create and disseminate surveys and the data from them, and is a very useful tool in this regard. The University of Michigan has a license, and so anyone at the university can use it, which is why I provide a generic example here. The qualtricsR13 package makes it easy to import data from and export surveys to Qualtrics, as well as functionality to create a standard survey within R. As before you’ll need proper authorization, but then it can be straightforward to grab your data from Qualtrics without having to go to the website. library(qualtricsR) mydata = importQualtricsData(username = &quot;qualtricsUser@email.address#brand&quot;, # example micl@umich.edu#umich token = &quot;tokenString&quot;, surveyID = &quot;idString&quot;) # your token from Qualtrics Others Note that there are many R packages that work with various APIs, so if you’re looking to work with data from a site that has an API, definitely see if something already exists. If it is a very popular website, you are probably not the first person using R that wants to access its data. Issues Documentation One issue I commonly find with APIs is that often they are generally poorly documented beyond a certain level of initial detail, e.g. often telling you what the parameters are but not the values they can take on. As an example, if something says it wants a date, you may be left to figure out what date format is expected. Sometimes I feel that developers spend so little effort it seems they don’t actually want people to use the API. Just be aware that you may still have some guesswork left even when a lot of information is provided initially. API Changes Many websites cannot leave the API alone very long. Sometimes this is due to actual content changes that require subsequent changes in the API, sometimes it might reflect a server side issue that needs to be addressed. Often a reason isn’t given. The gist is that you shouldn’t be surprised if an API package in R doesn’t work if it hasn’t been updated recently. As the code is often pretty straightforward, you may be able to easily tweak a function to work with the current API. Broken API Sometimes someone fiddles with things they shouldn’t and you start getting errors. As an example, when I was first developing this document, NY times article search would start providing different errors depending on the combination of start and end date (forbidden, API rate exceeded, no content etc.). It is not necessarily the case that you are doing something incorrectly. Restrictions Note that there are almost always restrictions. For example, the New York Times API limits requests to 1000 calls per day, and five calls per second. Unfortunately these are typically not based on actual testing of what modern servers could handle and are often overly restrictive (and overly optimistic in estimates of web traffic) in my opinion. I mention it though because you’ll need to plan ahead. If you need 1 million requests, but the API restricts you to 1000 a day, you’ll obviously need some other way to get the data you want. Fortunately, a simple request to the website developers for more flexibility might be all that is needed. APIs are much more general than just the Web version described here, in that they allow a means of interaction with anything- operating system, software, hardware etc. For our purposes though, it usually boils down to the examples above.↩ Technically uniform resource locator.↩ The reason you don’t see the key is that, like passwords, you should not provide your keys to others. They provide a means for you to interact with the website, not other people.↩ There are several APIs for different types of content.↩ https://github.com/saberry/qualtricsR↩ "],
["visualization.html", "Visualization Systems htmlwidgets Shiny Dashboards", " Visualization For standard approaches, R already is one of the more powerful data visualization tools out there, to the point where the look of its graphics is being replicated across other platforms. However, R has come a long way very quickly with regard to interactive and web-based visualization, such that it is as easy to do an interactive, web-ready graph as it is the standard plot. The following provides an overview of tools to be aware of for web-based visualization in R. Systems This section notes a couple packages which I will call visualization systems. For example, for static plots ggplot2 and lattice are systems within which to produce visualizations. They have their own consistent style with which one may specify the details of a plot and are very flexible in what they can produce. In that sense I’ll note two interactive visualization systems, Plotly and Bokeh. plotly Plotly is a general visualization system with APIs for JavaScript, Python, Matlab, and of interest to us, R. It used to require an account, but now is used as any other package within R, and in this case the package is just called plotly. Like ggplot2, you start with a base and add layers via pipes. library(plotly) plot_ly(x=~x, y=~y, color=factor(g), type=&#39;scatter&#39;, mode=&#39;markers&#39;) %&gt;% # add_trace() %&gt;% add_lines(y=y, line=list(shape=&#39;spline&#39;)) %&gt;% lazerhawk::theme_plotly() iris %&gt;% plot_ly(x=~Petal.Length, y=~Petal.Width, color=~Species) By default the plot is interactive with hover-over information, but you can add a lot more too it (my own theme cleans up gridlines, zero reference line, etc.). One of the best things about plotly is it’s ability to make a ggplot graphic interactive. The resulting visual may need further tweaking as there is not a one-to-one mapping from ggplot2 to plotly, but it’s otherwise a very useful tool. ## ggplot(aes(x=Petal.Length, y=Petal.Width, color=Species), data=iris) + ## geom_point() g = ggplot(aes(x=Petal.Length, y=Petal.Width, color=Species), data=iris) + geom_point() ggplotly(g) Unfortunately the documentation for plotly is very poor in my opinion. Every page on the website requires loading images that still take time even on very fast connections, there is so much unused whitespace that it can be difficult to tell which part of the functionality you’re reading about, examples are not consistent from language to language even for things that should not require different demonstrations, there are almost no comments explaining the code, etc. In short it can be frustrating to learn, and often you may have to go to the Python or JavaScript examples to get more detail than the R example provides. And of course, there is always SO. rbokeh Bokeh is a Python interactive visualization library, and rbokeh is an attempt to port it to the R world. It works very similarly to plotly and other packages, and for easy comparison I iterate the iris example from above. The figure function serves to produce the base of the plot, with other elements added as layers (ly_*) via pipes. It doesn’t appear to mesh well with bookdown yet, so I simply show the image you’d see in your RStudio viewer. library(rbokeh) iris %&gt;% figure(width=500, height=500) %&gt;% ly_points(Petal.Length, Petal.Width, data = iris, color = Species, glyph = Species, hover = list(Petal.Length, Petal.Width)) More on rbokeh can be found here. Others I’ll note that ggvis was once considered to be the successor to ggplot2, providing all the same stuff but specifically to add interactivity and web-based visualization. Development has stalled, probably due to the fact that things like Plotly and Bokeh attempt the same thing, but are used in other programming languages as well, and may have more development behind them. I do not recommend spending any time learning ggvis until development picks back up, and even then, plotly and rbokeh will likely be more developed and still have the cross-language advantage for a time. htmlwidgets Many newer visualization packages take advantage of JavaScript for graphical web-based display, as well as the piping approach to add layers of visual information to a plot. htmlwidgets is a package that makes it easy to create JavaScript visualizations, similar to what you see everywhere on the web. The packages using it typically are pipe-oriented (%&gt;%) and specifically produce interactive plots, and include plotly and rbokeh. Leaflet example The following example utilizes the leaflet package. Aside from the icons, you can create this same interactive graph from essentially nothing. As with most graphing approaches in R, we start with a base, which may or may not contain additional layout or other information. From there we add subsequent layers of information/visuals until the final goal is reached. The final product is an interactive map with popups when the icons are clicked. Rlogo = file.path(R.home(&#39;doc&#39;), &#39;html&#39;, &#39;logo.jpg&#39;) points = cbind(lon=c(-83.738233,-83.7582789), lat=c(42.280799, 42.2668577)) # latitude and longitude locations library(leaflet) leaflet() %&gt;% # base plot nothing to see addTiles() %&gt;% # base plot, the whole world setView(mean(points[,1]), mean(points[,2]), zoom = 14) %&gt;% # set location addMarkers(points[,1], points[,2], # add points etc. icon=list(iconUrl=c(&#39;../img/Rlogo.png&#39;, &#39;img/UM.png&#39;), iconSize = c(75, 75)), popup = c(&#39;CSCAR&#39;, &#39;Big House&#39;)) See the htmlwidgets website for more examples and other packages. Shiny Shiny is a framework within which to use R to build interactive web-based applications. Essentially you can create a website14 that allows interactive data creation, analysis and visualization. While you can use it easily enough on your own machine, it will not work for the web unless you have access to a an actual machine that can serve as the Shiny server. shinyapps.io allows you to create 5 apps for free, with the possibility to pay for more. Server You write two parts of code to use Shiny, and neither is in a fashion in which one typically uses R. The server determines how inputs will produce outputs for the application. It actually may have some more or less standard R code, as one extracts parts of data, produces plots etc. However, most of it will be wrapped as an expression within another R function, which itself is wrapped in a main function called shinyServer. The following comes from one of the Shiny tutorials and demonstrates the server setup. Note that you can define the server and UI as separate R files that are then called by the runApp function. library(shiny) # Define server logic required to draw a histogram shinyServer(function(input, output) { # Expression that generates a histogram. The expression is # wrapped in a call to renderPlot to indicate that: # # 1) It is &quot;reactive&quot; and therefore should be automatically # re-executed when inputs change # 2) Its output type is a plot output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] # Old Faithful Geyser data bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) }) }) UI The UI stands for user interface and defines how someone will interact with the inputs and the look of the page. This is where you essentially create the webpage where the user can adjust settings or other aspects which might then change an analysis, visualization or whatever. This is also where you will likely find more of the R code used in a fashion unlike your normal usage. And I hope you like parentheses. In the following the shinyUI function, like the shinyServer function before, wraps the rest of the code. Additional functions are used to create sidebars, interactive inputs, titles etc. # Define UI for application that draws a histogram shinyUI(fluidPage( # Application title titlePanel(&quot;Hello Shiny!&quot;), # Sidebar with a slider input for the number of bins sidebarLayout( sidebarPanel( sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) ) )) To see the app in action, simply install the shiny package and run the following code: shiny::runExample(&#39;01_hello&#39;) The shiny framework is very powerful in terms of what it can do, but realize you are basically creating a web app using a language that wasn’t designed for general programming in that fashion, and which will require outside resources if you want to use it. It is very likely that other approaches would be more efficient and notably lighter weight15, resulting in a better user experience, which is what your number one priority should be if you are constructing an interactive app. That said, shiny can be a lot of fun, and useful for presentations too if running it from your own device. See the gallery for demos and more. Dashboards The flex_dashboard output format allows one to quickly create a dashboard type webpage complete with whatever interactivity you may want. In a sense it comes off as a Shiny shortcut, enabling you to create a webpage quickly and easily by letting the default mode do more of the work behind the scenes. Consider the following image. The only R code necessary to start building a dashboard with the layout on the right is not even seen, it’s just part of the underlying markdown that comes with it. Whatever you put in the R chunks will show up in the corresponding chart position. As an example, I’d never used flex_dashboard before and it only took part of an afternoon to get the data and produce the following dashboard that looks at tornadoes in the U.S. over time (click image to see the app). It’s still demonstrates the slowness that can come with a shiny app, but it was relatively easy to put together. All plots use plotly. In short, there’s a lot one can do in creating R visualizations for the web, and more tools are being developed regularly. You’ll have a lot at your disposal, so have some fun with it! A lot of folks these days seem to confuse what a software application is, and what a website is, to the point that many actually call what is akin to a URL shortcut an ‘app’ (e.g. the Netflix app just takes you to netflix.com, i.e. while you may be able to find such a thing to download for your phone, this does not constitute a software application: ). In general, software applications do not require the web, and a web app is basically a website that functions similar to a software application that might run on your desktop or other device. Shiny has many functions that essentially create the html, JavaScript etc. you would otherwise have written. Whether you want to call the result an ‘app’ is up to you, but using Shiny won’t necessarily qualify you to call yourself a software developer. This point is also made because nowadays Shiny is actually unnecessary to create interactive webpages/sites (e.g. this document does not utilize Shiny). If all you want is some basic interactivity, you may be able to do that with the other packages noted.↩ Shiny apps may take a while to load and may be slowly implemented.↩ "],
["publishing.html", "Publishing Publishing Languages Document Formats Presentations Other formats Other", " Publishing The publishing capabilities in R are phenomenal, and keep expanding all the time. From traditional document types (including pdf and MS Word) to building whole websites with loads of interactivity, R can generally take you where you want to go. The ability to embed the data in the document is clearly important in academia for the facilitation of ideas and discovery, as well as conducting more reproducible research. However, such capabilities can benefit anyone. Publishing Languages Markdown Markdown serves as the basis for much of the approach. Markdown provides an easy way to create html products without coding any raw html. It is extremely limited in this regard, but for most text it’s all you need. Despite it’s utility, there is no standard for markdown and it hasn’t been developed in years. Thus there are many flavors of markdown which do continue to be developed, of which R Markdown is one. R Markdown R Markdown is Markdown with some other stuff that allows you to work with R. Like shiny is for webpages, it’s a framework for authoring with data science in mind. You no longer ever need your documents and data to be separated16. The basic process is such that you write an R Markdown document and it is then converted into the desired format via other tools, e.g. html or pdf. Use File/New File/R Markdown... in RStudio to get started. The following shows what a markdown file might look like. You see YAML for the first few lines, basic text intermingled with standard markdown (e.g. ** adds bold, # are headers), and R chunks, where R code resides. The R code itself may remain hidden, exposed but not run, or run along with the rest of the output and the results shown in the text. Much of this process is made possible via the knitr package, which takes the .Rmd file and knits it into the desired format, using things like pandoc and other tools to convert what you to see into the final product. You can always change the output format, so it doesn’t really matter what you select at the initial point. It might be obvious, but you’ll need outside programs for some things. For example, you can’t knit a MS Word doc17 if MS Word is not installed on your computer. Likewise, you’ll need a \\(\\LaTeX\\) installation to create a pdf. You should also know that what a document renders to in html will not have the exact same look in pdf or MS Word, nor could it be expected to. If you select an inferior or less flexible format for your publication, don’t expect all the bells and whistles that worked in a better format to come along. HTML, CSS etc. HTML is what is behind most of the web that you actually see and interact with, allowing one to create a webpage in a mostly tabular format of some kind. CSS allows you have a consistent style across a collection of pages usually amalgamated to form an entire website. Javascript allows one to build applications that run within the browser, and a host of other languages process data on the server side. You don’t need to know these languages to produce documents with R, but the more you know of them the more you can enhance and customize your product. LaTeX It used to be the case that \\(\\LaTeX\\) served as the primary means for producing high-quality documents in academia, and with a lot of headache, it can make beautiful work. However, despite some impressions, these days, pdf and similar print-first output is neither necessary nor really should be the chief means of scientific communication, and so the need of \\(\\LaTeX\\) has been greatly minimized. It is still useful for formulas, but even that is translated via Mathjax, which is a JavaScript library that essentially reproduces the math functionality of \\(\\LaTeX\\). For example, inserting the following bit in an R Markdown document $$y = X\\beta + \\epsilon$$ will produce the following when the document is ‘knit’: \\[y = X\\beta + \\epsilon\\] One can also use various latex packages as well if needed. However, with R Markdown, using raw \\(\\LaTeX\\) is rarely needed outside of formal mathematical exposition. There are several packages such as xtable that may be able to create a \\(\\LaTeX\\) table for you, or perhaps there are other means to display things that might be better. Thinking beyond the restriction of page boundaries and printed work will take time to get used to, but is well worth it. YAML YAML (or YAML Aint Markup Language) serves as the means to configure your R Markdown files18. The syntax is very simple, and in the context of an R Markdown document, it allows you to specify things like the output type, title, other css files to use, and so forth. This will start any R Markdown file you use unless you have one main file that calls other Rmd files. Pandoc Pandoc is a universal converter19, allowing markup languages to flop from one format to another. Essentially a babel fish for web, it reads the markdown, HTML, \\(\\LaTeX\\), and everything else and converts it to an HTML, pdf etc. document. From the pandoc website: Markdown, CommonMark, PHP Markdown Extra, GitHub-Flavored Markdown, MultiMarkdown, and (subsets of) Textile, reStructuredText, HTML, LaTeX, MediaWiki markup, TWiki markup, Haddock markup, OPML, Emacs Org mode, DocBook, txt2tags, EPUB, ODT and Word docx; and it can write plain text, Markdown, CommonMark, PHP Markdown Extra, GitHub-Flavored Markdown, MultiMarkdown, reStructuredText, XHTML, HTML5, LaTeX (including beamer slide shows), ConTeXt, RTF, OPML, DocBook, OpenDocument, ODT, Word docx, GNU Texinfo, MediaWiki markup, DokuWiki markup, ZimWiki markup, Haddock markup, EPUB (v2 or v3), FictionBook2, Textile, groff man pages, Emacs Org mode, AsciiDoc, InDesign ICML, TEI Simple, and Slidy, Slideous, DZSlides, reveal.js or S5 HTML slide shows. It can also produce PDF output on systems where LaTeX, ConTeXt, or wkhtmltopdf is installed. Summary As mentioned you don’t have to know these languages to produce really interesting documents. However, being more cognizant of their capabilities means you’ll be able to be more creative with your products. Document Formats HTML The html_document is the default output for R Markdown documents, and really should be your default as well. It provides much more freedom and easier specificity in the way content is presented, and can get quite extensive. Notebooks I’ve been using notebooks since they were first advertised as part of an RStudio preview release. However, it is still not clear to me what their role is20, other than to allow your output to reside in your R markdown document rather than the console, something typically not desirable to me (at least with how I use R). I guess it’s useful if you want to share the script/document and let someone else play with the code, but technically they could do that with an R Markdown document anyway, or a basic R script for that matter. Having the output below or to the side of the document rather than a few inches from that point is not something I can get overly excited about, and you can do that with the standard html_document format. And sometimes there is a lot of output from a chunk that I definitely do not want clogging up the code or document, but still may want to regularly inspect. Code should have a flow just like text should, at least that’s my opinion. That said, notebooks do appear be highly useful for educational purposes, and more so if they make it such that the notebooks run in Jupyter21 similar to Python’s *.nb files. I also like Julia’s approach to it’s standard script, where you can simply hover/click for the output rather than have it automatically displayed, and maybe the R notebooks will go that direction. Stay tuned to see how this document format pans out. Print For more print-oriented approaches you have pdf, MS Word, RTF, ODF. As mentioned, you will need to have something else installed (e.g. a \\(\\TeX\\) installation for pdf) in order to use these. Furthermore, you should think very hard about whether you need them, as you will sacrifice your document’s capabilities and look in some fashion. Journals There is some template support for Elsevier journals (assuming you’re not boycotting them) and some specific ones that the R crowd would be interested in such as the Journal of Statistical Software. More may be available via packages or added over time. My impression is that it’s more likely the journals are catering to the output (e.g. some already accept markdown files), such that the templates may not be necessary. Presentations Some of the presentations for my CSCAR workshops use revealjs22 or similar for a web-based approach, though I find slides highly restrictive for conveying information that isn’t visual. For better or worse, at present there are multiple types of presentations one might use in R with varying degrees of functionality. ioslides: HTML presentation with ioslides reveal.js: HTML presentation with reveal.js (requires revealjs package/template) Slidy: HTML presentation with W3C Slidy Beamer: PDF presentation with \\(\\LaTeX\\) Beamer These days I cannot think of a reason to do a set of pdf slides as there is zero benefit and an automatic loss of functionality. The others are more configurable and can work on any device as easily. Given that you can run something like an interactive shiny app as part of a presentation, why would you give that up?23 As for the others, they will all serve you well but seem intermittently developed aside from the revealjs. Be aware that just because you did it with a standard html_document format, does not mean it will work just the same when trying to make a slide form of it24. Other formats bookdown HTML, PDF, ePub, and Kindle books Websites Multi-page websites. Tufte Handout Handouts in the style of Edward Tufte Package Vignette R package vignette (HTML) GitHub Document GitHub Flavored Markdown document. I’ll provide a few words about these formats but may expand later. This document was created with bookdown, so that should give you some idea about its capabilities. The website format is not shiny, nor just the standard html doc output, but a format of its own. The Tufte handouts work very well for pdf, and perhaps for standard html versions, but I’ve had issues with bookdown and the Tufte output is not well documented for it. Package vignettes are very useful if you actually create an R package, but can otherwise be ignored. A GitHub doc isn’t much different from standard markdown, so may be of limited use, even assuming you are using GitHub in the first place. Other Customization For customization you’ll need to learn at least a little HTML and CSS, and possibly quite a bit once you go down the rabbit hole, along with other languages such as JavaScript. This goes for all R markdown based document formats. I find it necessary to get things how I want them, but you may be fine with default themes etc. In addition, user-created formats with custom looks or functionality are now coming into play, and will provide other formats beyond what you can get from the RStudio crowd. Rpubs With an Rpubs account, you can publish your documents directly to the web for easy dissemination. This allows you to share any of your R creations with ease. For those in academia this is perhaps of limited use, as typically everyone is given their own web space already. However, even then it could potentially serve as an additional outlet. More I’ll add more stuff here as I come across it. Unfortunately, many journals still seem to think it’s 1985 where people mostly access them in print form in the library. These are also journals that aren’t being cited as much anymore. Accessibility and openness are the hallmarks of science, and any journal outlets that have not figured this out should not be allowed to ride the coattails of their past status.↩ I’ve yet to come across any reason to still be using MS Word nowadays. It is easier to create an MS Word document via markdown than it is to use the program itself.↩ There are other things that YAML is that I do not understand (e.g. How ‘Yet Another Markup Language’ now doesn’t want to be considered a markup language).↩ Actually a Haskell library.↩ Maybe I’m short-sighted but notebooks strike me as the phablet of R markdown output formats. As quoted from J.J. Allaire in response from someone who wanted a simple description that states the difference between notebooks and the standard html doc: “They are the same thing, r notebooks are just a way of working interactively with R Markdown and embedding the source code directly within the R Markdown output.”↩ You can already use R with Jupyter, but it’s limited compared with what you can do with R in RStudio with R Markdown.↩ revealjs is an R package that ports the reveal JavaScript library.↩ R.I.P. Beamer.↩ It’s not clear to me why anything shouldn’t work aside from bounding box issues, but you’ll have notable problems with some interactive graphics, while others will work great.↩ "],
["conclusion.html", "Conclusion", " Conclusion As mentioned at the beginning of this document, do check the CRAN Task View for Web Technologies. There’s a lot more out there that allows you to use R with the web, and the capabilities increase all the time. The purpose of this document was to merely give insight into some possibilities for how to use R to engage the web, with simple demonstrations or explanations of those approaches. However, any discussed here can be taken further, and there is a great deal more to explore, so have some fun! "],
["references.html", "References", " References "]
]
